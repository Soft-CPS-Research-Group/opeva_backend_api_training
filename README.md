# ğŸš€ OPEVA Backend API Training

This repository contains the backend API service for managing the execution of simulations and algorithms in the OPEVA infrastructure.

The service provides a REST API to:
- Launch simulation jobs dynamically inside Docker containers
- Track job status
- Stream and persist container logs
- Track progress and collect results
- Stop running jobs if needed
- Maintain persistent tracking of jobs even after restarts
- Manage and delete configs or datasets
- âœ¨ Stream training log files from simulation container (not just stdout)
- Retrieve data from MongoDB collections (Living Lab and iCharging Headquarters)
- Create datasets from MongoDB data with per-building and per-EV CSVs

The backend is fully integrated with:
- **OPEVA shared data storage** (`/opt/opeva_shared_data/`)
- **OPEVA Docker network** (`opeva_network`)
- **MLflow tracking server** (automatic metrics reporting)
- **Watchtower** for automatic CI/CD updates

---

## ğŸ“¦ Project Structure

```
opeva_backend_api_training/
app/
â”œâ”€â”€ api/                        # Routes (API layer)
â”‚   â”œâ”€â”€ endpoints/
â”‚   â”‚   â”œâ”€â”€ jobs.py
â”‚   â”‚   â”œâ”€â”€ configs.py
â”‚   â”‚   â”œâ”€â”€ datasets.py
â”‚   â”‚   â”œâ”€â”€ mongo.py
â”‚   â”‚   â””â”€â”€ health.py
â”‚   â””â”€â”€ router.py               # Main APIRouter() mounting subroutes
â”œâ”€â”€ controllers/               # Controllers - HTTP-facing logic
â”‚   â”œâ”€â”€ job_controller.py
â”‚   â”œâ”€â”€ config_controller.py
â”‚   â”œâ”€â”€ dataset_controller.py
â”‚   â””â”€â”€ mongo_controller.py
â”œâ”€â”€ services/                  # Business logic
â”‚   â”œâ”€â”€ job_service.py
â”‚   â”œâ”€â”€ config_service.py
â”‚   â”œâ”€â”€ dataset_service.py
â”‚   â””â”€â”€ mongo_service.py
â”œâ”€â”€ models/                    # Pydantic models and domain entities
â”‚   â””â”€â”€ job.py
â”œâ”€â”€ utils/                     # Low-level utilities
â”‚   â”œâ”€â”€ docker_manager.py
â”‚   â”œâ”€â”€ job_utils.py
â”‚   â”œâ”€â”€ file_utils.py
â”‚   â””â”€â”€ mongo_utils.py
â”œâ”€â”€ config.py
â”œâ”€â”€ main.py                    # Just mounts router
â”œâ”€â”€ Dockerfile                # Containerization
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ .github/workflows/        # GitHub Actions CI/CD
â”‚   â””â”€â”€ docker-publish.yml
â”œâ”€â”€ README.md                 # This file
```

---

## ğŸ§¹ Infrastructure Integration

This service is part of the **OPEVA Infra Services** stack and communicates with:
- MLflow: `http://mlflow:5000`
- Simulation services: dynamically launched containers
- Shared storage: `/opt/opeva_shared_data/`
- MongoDB databases: `living_lab` and `i-charging_headquarters`

The service attaches to the external Docker network:
```
networks:
  opeva_network:
    external: true
```

It uses the shared `/opt/opeva_shared_data/` folder to store:

- All outputs (logs, results, progress, metadata) are stored under `/jobs/{job_id}/`, including:
  - `logs/{job_id}.log`
  - `results/result.json`
  - `progress/progress.json`
  - `job_info.json`

## Getting Started
### Requirements
- Docker
- Docker Compose
- Docker network: `opeva_network` (external, global)
- Shared data folder: `/opt/opeva_shared_data/`

**Build and run locally**
```bash
docker build -t opeva_backend_api_training .
docker run -p 8000:8000 --network opeva_network -v /opt/opeva_shared_data:/data opeva_backend_api_training
```

**Using Docker Compose**
```bash
cd /opt/opeva_infra_services/opeva_backend_api
docker-compose up -d
```
This will start:
- The backend API on port 8000
- Watchtower for automatic deployment updates

## API Overview

| Method | Endpoint                                | Description                                                                 |
|--------|-----------------------------------------|-----------------------------------------------------------------------------|
| GET    | /sites                                  | List available MongoDB databases (each representing a "site")              |
| GET    | /real-time-data/{site_name}             | Retrieve all collections and documents from the specified MongoDB site     |
| GET    | /real-time-data/{site_name}?minutes=X   | Retrieve only documents from the last X minutes across all collections     |
| POST   | /run-simulation                         | Launch a new simulation job                                                |
| GET    | /status/{job_id}                        | Check job status                                                           |
| GET    | /result/{job_id}                        | Get final results of job                                                   |
| GET    | /progress/{job_id}                      | Get progress updates                                                       |
| GET    | /logs/{job_id}                          | Stream container logs                                                      |
| GET    | /logs/file/{job_id}                     | Stream simulation log file (.log)                                          |
| POST   | /stop/{job_id}                          | Stop a running container/job                                               |
| GET    | /jobs                                   | List all tracked jobs                                                      |
| GET    | /job-info/{job_id}                      | Get metadata about a job                                                   |
| DELETE | /job/{job_id}                           | Delete job and its files                                                   |
| GET    | /health                                 | Health check of the API                                                    |
| POST   | /config/create                          | Create new config file                                                     |
| GET    | /configs                                | List all config files                                                      |
| GET    | /config/{file}                          | View a config file                                                         |
| DELETE | /config/{file}                          | Delete a config file                                                       |
| POST   | /dataset                                | Create a new dataset from a MongoDB site (buildings + EVs to CSVs)         |
| GET    | /datasets                               | List all available datasets                                                |
| DELETE | /dataset/{name}                         | Delete a dataset and its contents                                          |
| GET    | /hosts                                  | List all available hosts                                                   |

---

## CI/CD Pipeline

This repository uses GitHub Actions to build and publish Docker images to GitHub Container Registry:
```
.github/workflows/docker-publish.yml
```
On every push to `main`:
- Docker image is built and pushed to `ghcr.io/tiagofonseca/opeva_backend_api_training:latest`
- Watchtower running in the VM will automatically detect updates and redeploy the service

**Polling interval for Watchtower**: every 24 hours (`WATCHTOWER_POLL_INTERVAL=86400`)

## Persistent Job Tracking
Job container IDs are saved across reboots:
```
/opt/opeva_shared_data/job_track.json
```

## Logs and Results
Simulation outputs are persisted under `/opt/opeva_shared_data/jobs/{job_id}/`:
- Logs: `logs/{job_id}.log`
- Results: `results/result.json`
- Progress: `progress/progress.json`
- Metadata: `job_info.json`

---

## ğŸ˜“ Best Practices & Gotchas

âœ… Always point to `/data/` for datasets/configs inside containers  
âœ… Use inline config when launching dynamically from UI  
âœ… Always mount `/opt/opeva_shared_data` as `/data` in containers  

âŒ Do **not** use relative paths like `./datasets/...` in configs  
âŒ Do **not** rely on container stdout logs â€” use the generated `.log` files

---

## ğŸ“ Output Paths (in /opt/opeva_shared_data)
```
jobs/{job_id}/
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ {job_id}.log
â”œâ”€â”€ progress/
â”‚   â””â”€â”€ progress.json
â”œâ”€â”€ results/
â”‚   â””â”€â”€ result.json
â”œâ”€â”€ job_info.json
```

---

## ğŸ“° Full API Usage Examples

### âœ… Launch a Simulation (existing config)
```bash
curl -X POST http://<IP>:8000/run-simulation \
  -H "Content-Type: application/json" \
  -d '{
    "config_path": "configs/my_config.yaml",
    "target_host": "local"
}'
```

### âœ… Launch a Simulation (inline config)
```bash
curl -X POST http://<IP>:8000/run-simulation \
  -H "Content-Type: application/json" \
  -d '{
    "config": { ... },
    "save_as": "generated_config.yaml",
    "target_host": "local"
}'
```

### ğŸ” Check Job Status
```bash
curl http://<IP>:8000/status/{job_id}
```

### ğŸ“Š Get Simulation Results
```bash
curl http://<IP>:8000/result/{job_id}
```

### ğŸ“ˆ Get Training Progress
```bash
curl http://<IP>:8000/progress/{job_id}
```

### ğŸ“„ Stream Logs (stdout)
```bash
curl http://<IP>:8000/logs/{job_id}
```

### ğŸŸï¸ Stream Training Log File (.log)
```bash
curl http://<IP>:8000/logs/file/{job_id}
```

### âŒ Stop Running Job
```bash
curl -X POST http://<IP>:8000/stop/{job_id}
```

### ğŸ“ƒ List Jobs
```bash
curl http://<IP>:8000/jobs
```

### ğŸ“° Job Metadata
```bash
curl http://<IP>:8000/job-info/{job_id}
```

### Available Hosts
```bash
curl http://<IP>:8000/hosts
```

### â¤ï¸ Health Check
```bash
curl http://<IP>:8000/health
```

---

## ğŸ”§ Configs & Datasets Management

### âœ… Create Config
```bash
curl -X POST http://<IP>:8000/config/create \
  -H "Content-Type: application/json" \
  -d '{
    "file_name": "custom.yaml",
    "config": { ...config... }
}'
```

### ğŸ“œ List Configs
```bash
curl http://<IP>:8000/configs
```

### ğŸ” View Config File
```bash
curl http://<IP>:8000/config/custom.yaml
```

### âŒ Delete Config File
```bash
curl -X DELETE http://<IP>:8000/config/custom.yaml
```

### âœ… Create Dataset (from MongoDB site)
```bash
curl -X POST http://<IP>:8000/dataset \
  -H "Content-Type: application/json" \
  -d '{
    "name": "dataset1",
    "site_id": "living_lab",
    "config": {
      "parameter1": "value1",
      "parameter2": 42
    },
    "from_ts": "2023-01-01 00:00:00",
    "until_ts": "2023-01-01 00:00:00"
}'
```

### ğŸ“ƒ List Datasets
```bash
curl http://<IP>:8000/datasets
```

### âŒ Delete Dataset
```bash
curl -X DELETE http://<IP>:8000/dataset/dataset1
```

### âŒ Delete Job (and its folder)
```bash
curl -X DELETE http://<IP>:8000/job/{job_id}
```

## ğŸ“° MongoDB Endpoints Usage Examples

### âœ… List all available MongoDB sites (databases)
```bash
curl http://<IP>:8000/sites
```

### âœ… Retrieve all real-time data from a specific site (e.g., iCharging Headquarters)
```bash
curl http://<IP>:8000/real-time-data/i-charging_headquarters
```

### âœ… Retrieve only the last 60 minutes of data from a specific site
```bash
curl "http://<IP>:8000/real-time-data/living_lab?minutes=60"
```

---

